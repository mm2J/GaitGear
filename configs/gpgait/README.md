

# GPGait
GPGait: Generalized Pose-based Gait Recognition (ICCV 2023)

## Abstract
Recent works on pose-based gait recognition have demonstrated the potential of using such simple information to achieve results comparable to silhouette-based methods. However, the generalization ability of pose-based methods on different datasets is undesirably inferior to that of silhouette-based ones, which has received little attention but hinders the application of these methods in real-world scenarios. To improve the generalization ability of pose-based methods across datasets, we propose a Generalized Pose-based Gait recognition (GPGait) framework. First, a Human-Oriented Transformation (HOT) and a series of Human-Oriented Descriptors (HOD) are proposed to obtain a unified pose representation with discriminative multi-features. Then, given the slight variations in the unified representation after HOT and HOD, it becomes crucial for the network to extract local-global relationships between the keypoints. To this end, a Part-Aware Graph Convolutional Network (PAGCN) is proposed to enable efficient graph partition and local-global spatial feature extraction. Experiments on four public gait recognition datasets, CASIA-B, OUMVLP-Pose, Gait3D and GREW, show that our model demonstrates better and more stable cross-domain capabilities compared to existing skeleton-based methods, achieving comparable recognition results to silhouette-based ones. 
<div align="center"><img src="..\..\resources\gpgait_pipeline.png"  alt="logo" /></div>

## Results
<div align="center">
<table border="1" style="margin: auto; text-align: center;" width='100%'>
    <tr>
        <td rowspan = "2" >Source</td>
        <td rowspan = "2">Method</td>
        <td colspan = "4">Target</td>
    </tr>
    <tr>
        <td>CASIA-B</td>
        <td>OUMVLP-Pose</td>
        <td>GREW</td>
        <td>Gait3D</td>
    </tr>
    <tr>
        <td rowspan = "4">CASIA-B</td>
        <td>GaitGraph</td>
        <td>76.04</td>
        <td>0.07</td>
        <td>0.45</td>
        <td>0.9</td>
    </tr>
    <tr>
        <td>GaitGraph2</td>
        <td>71.83</td>
        <td>0.07</td>
        <td>0.48</td>
        <td>1.1</td>
    </tr>
    <tr>
        <td>GaitTR</td>
        <td>90.22</td>
        <td>0.07</td>
        <td>0.62</td>
        <td>1.1</td>
    </tr>
    <tr>
        <td>GPGait</td>
        <td>81.01</td>
        <td>2.84</td>
        <td>9.97</td>
        <td>8.9</td>
    </tr>
    <tr>
        <td rowspan = "4">OUMVLP-Pose</td>
        <td>GaitGraph</td>
        <td>4.53</td>
        <td>4.24</td>
        <td>0.67</td>
        <td>1.5</td>
    </tr>
    <tr>
        <td>GaitGraph2</td>
        <td>7.19</td>
        <td>70.68</td>
        <td>0.85</td>
        <td>1.4</td>
    </tr>
    <tr>
        <td>GaitTR</td>
        <td>7.85</td>
        <td>39.77</td>
        <td>1.06</td>
        <td>2.6</td>
    </tr>
    <tr>
        <td>GPGait</td>
        <td>32.9</td>
        <td>59.11</td>
        <td>11.13</td>
        <td>9</td>
    </tr>
    <tr>
        <td rowspan = "4">GREW</td>
        <td>GaitGraph</td>
        <td>8</td>
        <td>0.17</td>
        <td>10.18</td>
        <td>4.4</td>
    </tr>
    <tr>
        <td>GaitGraph2</td>
        <td>7.05</td>
        <td>0.22</td>
        <td>34.78</td>
        <td>8.3</td>
    </tr>
    <tr>
        <td>GaitTR</td>
        <td>6.79</td>
        <td>0.06</td>
        <td>48.58</td>
        <td>7.3</td>
    </tr>
    <tr>
        <td>GPGait</td>
        <td>42.69</td>
        <td>4.25</td>
        <td>57.04</td>
        <td>18.5</td>
    </tr>
    <tr>
        <td rowspan = "4">Gait3D</td>
        <td>GaitGraph</td>
        <td>12.31</td>
        <td>0.27</td>
        <td>3.14</td>
        <td>8.6</td>
    </tr>
    <tr>
        <td>GaitGraph2</td>
        <td>9.23</td>
        <td>0.09</td>
        <td>2.39</td>
        <td>11.2</td>
    </tr>
    <tr>
        <td>GaitTR</td>
        <td>4.12</td>
        <td>0.06</td>
        <td>4.38</td>
        <td>7.2</td>
    </tr>
    <tr>
        <td>GPGait</td>
        <td>36.17</td>
        <td>2.79</td>
        <td>11.02</td>
        <td>22.4</td>
    </tr>
</table>
</div>

* In the literature, there are two versions of pose-based CASIA-B estimated by HRNet and SimCC, respectively. Given that Gait3D and GREW are generated by HRNet, we finally use the HRNet version of CASIA-B for a unified experimental setting.
* For OUMVLP-Pose, the sequences are generated by AlphaPose consisting of 18 keypoints for each frame. In our experiments, we transform the keypoints into the COCO2017 format with 17 keypoints for the cross-domain evaluation.

## Citation
```
@InProceedings{Fu_2023_ICCV,
    author    = {Fu, Yang and Meng, Shibei and Hou, Saihui and Hu, Xuecai and Huang, Yongzhen},
    title     = {GPGait: Generalized Pose-based Gait Recognition},
    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
    month     = {October},
    year      = {2023},
    pages     = {19595-19604}
}
```
